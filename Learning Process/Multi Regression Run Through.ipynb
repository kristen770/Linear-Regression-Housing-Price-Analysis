{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = full_cleaned[full_cleaned['price'] < 950000]\n",
    "print('Percent removed:',(len(full_cleaned) - len(subset))/len(full_cleaned)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at sqft_living values\n",
    "raw_df.groupby(['zipcode']).agg(['median','mean', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#homoskadaticy check \n",
    "plt.scatter(model.predict(train[x_cols]), model.resid)\n",
    "plt.plot(model.predict(train[x_cols]), [0 for i in range(len(train))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the p-value table from the summary and use it to subset our features\n",
    "summary = model.summary()\n",
    "p_table = summary.tables[1]\n",
    "p_table = pd.DataFrame(p_table.data)\n",
    "p_table.columns = p_table.iloc[0]\n",
    "p_table = p_table.drop(0)\n",
    "p_table = p_table.set_index(p_table.columns[0])\n",
    "p_table['P>|t|'] = p_table['P>|t|'].astype(float)\n",
    "x_cols = list(p_table[p_table['P>|t|'] < 0.05].index)\n",
    "x_cols.remove('Intercept')\n",
    "print(len(p_table), len(x_cols))\n",
    "print(x_cols[:5])\n",
    "p_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for linearity of features \n",
    "sns.jointplot('price','bathrooms', data=cleaned, kind='reg'); \n",
    "sns.jointplot('price','bedrooms', data=cleaned, kind='reg'); \n",
    "sns.jointplot('price','condition', data=cleaned, kind='reg');  \n",
    "sns.jointplot('price','grade', data=cleaned, kind='reg'); \n",
    "sns.jointplot('price','sqft_living', data=cleaned, kind='reg'); \n",
    "sns.jointplot('price','floors', data=cleaned, kind='reg'); \n",
    "sns.jointplot('price','sqft_above', data=cleaned, kind='reg');  \n",
    "sns.jointplot('price','sqft_living15', data=cleaned, kind='reg'); \n",
    "#checking for multicollinearity  \n",
    "feats = ['bathrooms', 'bedrooms', 'condition', 'grade', 'sqft_living', 'floors',  'sqft_above', 'sqft_living15']\n",
    "corr = df[feats].corr()\n",
    "corr \n",
    "\n",
    "#considering r > 0.65 as multicolinear bathrooms & grade appears to be the only multicolinear item?   \n",
    "sns.heatmap(corr, center=0, annot=True) \n",
    "Initial Model\n",
    "I would like to look at what renovatable (changable) feature most affects housing price. I am choosing to look only at \n",
    "features that could be altered i.e. not view, zipcode, waterfront, yr_built, zipcode, lat, long, sqft_lot15 since those\n",
    "features cannot be changed. I am modeling my data this way because my stake holder in this project is a nonprofit/ city\n",
    "partnership looking to remodel houses at affordable prices & build equity in communities. \n",
    "\n",
    "#The problem\n",
    "outcome = 'price'\n",
    "x_cols = ['bathrooms', 'bedrooms', 'condition', 'grade', 'sqft_living', 'floors',  'sqft_above', 'sqft_living15']  \n",
    "\n",
    "#create a seprate df before changing values \n",
    "inital_df = df.copy()\n",
    "\n",
    "for col in x_cols: \n",
    "    inital_df[col] = (inital_df[col] - df[col].mean())/inital_df[col].std()\n",
    "inital_df.head() \n",
    "\n",
    "#fitting the model \n",
    "predictors = '+'.join(x_cols)\n",
    "formula = outcome + '~' + predictors \n",
    "model = ols(formula=formula, data=inital_df).fit()\n",
    "model.summary() \n",
    "\n",
    "#variance inflation factor  \n",
    "X = inital_df[x_cols] \n",
    "vif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])] \n",
    "list(zip(x_cols, vif)) \n",
    "\n",
    "#update model \n",
    "outcome = 'price'\n",
    "x_cols =['bathrooms', 'bedrooms', 'condition', 'grade', 'sqft_living', 'floors',  'sqft_above', 'sqft_living15'] \n",
    "predictors = '+'.join(x_cols)\n",
    "formula = outcome + '~' + predictors \n",
    "model = ols(formula=formula, data=inital_df).fit()\n",
    "model.summary() \n",
    "\n",
    "#pval of 0.0 means that we are certain this relationship is not due to chance.  \n",
    "Normalicy Check \n",
    "fig = sm.graphics.qqplot(model.resid, dist=stats.norm, line='45', fit=True) \n",
    "Homoscedacity Check \n",
    "plt.scatter(model.predict(inital_df[x_cols]), model.resid)\n",
    "plt.plot(model.predict(inital_df[x_cols]), [0 for i in range(len(inital_df))]) \n",
    "Model Refinement\n",
    "#data cut off point\n",
    "for i in range(90, 99): \n",
    "    q = i / 100 \n",
    "    print('{} percentile: {}'.format(q, inital_df['price'].quantile(q=q))) \n",
    "subset = inital_df[inital_df['price'] < 1260000]\n",
    "print('Percent removed:', (len(inital_df) - len(subset)) / len(df))\n",
    "outcome= 'price'\n",
    "x_cols = ['bathrooms', 'bedrooms', 'condition', 'grade', 'sqft_living', 'floors',  'sqft_above', 'sqft_living15'] \n",
    "predictors = '+'.join(x_cols)\n",
    "formula = outcome + '~' + predictors\n",
    "model = ols(formula=formula, data=subset).fit()\n",
    "model.summary() \n",
    "\n",
    "#recheck normality \n",
    "fig = sm.graphics.qqplot(model.resid, dist=stats.norm, line='45', fit=True) \n",
    "\n",
    "#recheck homostatisity \n",
    "plt.scatter(model.predict(subset[x_cols]), model.resid)\n",
    "plt.plot(model.predict(subset[x_cols]), [0 for i in range(len(subset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for correlation numerically\n",
    "subset7=X_train.corr().abs().stack().reset_index().sort_values(0, ascending=False)\n",
    "\n",
    "# zip the variable name columns (Which were only named level_0 and level_1 by default) in a new column named \"pairs\"\n",
    "subset['pairs'] = list(zip(mldf.level_0, mldf.level_1))\n",
    "\n",
    "# set index to pairs\n",
    "corr.set_index(['pairs'], inplace = True)\n",
    "\n",
    "#d rop level columns\n",
    "corr.drop(columns=['level_1', 'level_0'], inplace = True)\n",
    "\n",
    "# rename correlation column as cc rather than 0\n",
    "corr.columns = ['cc']\n",
    "\n",
    "# drop duplicates. This could be dangerous if you have variables perfectly correlated with variables other than themselves.\n",
    "# for the sake of exercise, kept it in.\n",
    "corr.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mldf[(mldf.cc>.75) & (mldf.cc <1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline model \n",
    "\n",
    "regression = LinearRegression()\n",
    "crossvalidation = KFold(n_splits=3, shuffle=True, random_state=1) \n",
    "    \n",
    "baseline= np.mean(cross_val_score(regression, mldf, mldf['price'], scoring='r2', cv=crossvalidation))\n",
    "print(\"Baseline:\", baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = z_score.drop('price', axis=1)\n",
    "y = z_score['price'] \n",
    "# Split the data into training and test sets (assign 20% to test set) \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (assign 20% to test set) \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A brief preview of train-test split\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = X_train\n",
    "y1 = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([X_train, y_train], axis=1, join='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin removing features that are overly correlated as indicated on the df.cc check above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial 2 \n",
    "Remove 'sqft_living15', 'sqft_above', 'sqft_living', 'lot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = train.drop([\"sqft_living15\", \"sqft_above\", \"sqft_living\", \"lot\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#redefine problem\n",
    "outcome = \"price\"\n",
    "t2 = ['bedrooms', 'bathrooms', 'sqft_lot', 'floors', 'view',\n",
    "       'condition', 'grade', 'sqft_basement', 'yr_built',\n",
    "       'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_lot15',\n",
    "       'wf', 'living', 'above', 'fliving', 'flot', 'basement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg(outcome=\"price\", x_cols=t2, df=train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_df2 = train1.corr() \n",
    "correlation_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(correlation_df2, center=0, annot=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.cc>.75) & (df.cc <1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = train.drop([\"living\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#redefine problem\n",
    "outcome = \"price\"\n",
    "t3 = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'view',\n",
    "       'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built',\n",
    "       'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15',\n",
    "       'wf', 'lot', 'above', 'fliving', 'flot', 'basement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg(outcome=outcome, x_cols=t3, df=train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That improved our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_df3 = train2.corr() \n",
    "correlation_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(correlation_df2, center=0, annot=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for correlation numerically\n",
    "df1=train2.corr().abs().stack().reset_index().sort_values(0, ascending=False)\n",
    "\n",
    "# zip the variable name columns (Which were only named level_0 and level_1 by default) in a new column named \"pairs\"\n",
    "df1['pairs'] = list(zip(df1.level_0, df1.level_1))\n",
    "\n",
    "# set index to pairs\n",
    "df1.set_index(['pairs'], inplace = True)\n",
    "\n",
    "#d rop level columns\n",
    "df1.drop(columns=['level_1', 'level_0'], inplace = True)\n",
    "\n",
    "# rename correlation column as cc rather than 0\n",
    "df1.columns = ['cc']\n",
    "\n",
    "# drop duplicates. This could be dangerous if you have variables perfectly correlated with variables other than themselves.\n",
    "# for the sake of exercise, kept it in.\n",
    "df1.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[(df1.cc>.75) & (df1.cc <1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trail 4  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train3 = train2.drop(['sqft_living15', 'sqft_above', 'lot'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#redefine problem\n",
    "outcome = \"price\"\n",
    "t4 = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'view',\n",
    "       'condition', 'grade', 'sqft_basement', 'yr_built', 'yr_renovated',\n",
    "       'zipcode', 'lat', 'long', 'sqft_lot15', 'wf', 'above', 'fliving',\n",
    "       'flot', 'basement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg(outcome=\"price\", x_cols=t4, df=train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for correlation numerically\n",
    "df2=train3.corr().abs().stack().reset_index().sort_values(0, ascending=False)\n",
    "\n",
    "# zip the variable name columns (Which were only named level_0 and level_1 by default) in a new column named \"pairs\"\n",
    "df2['pairs'] = list(zip(df2.level_0, df2.level_1))\n",
    "\n",
    "# set index to pairs\n",
    "df2.set_index(['pairs'], inplace = True)\n",
    "\n",
    "#d rop level columns\n",
    "df2.drop(columns=['level_1', 'level_0'], inplace = True)\n",
    "\n",
    "# rename correlation column as cc rather than 0\n",
    "df2.columns = ['cc']\n",
    "\n",
    "# drop duplicates. This could be dangerous if you have variables perfectly correlated with variables other than themselves.\n",
    "# for the sake of exercise, kept it in.\n",
    "df2.drop_duplicates(inplace=True) \n",
    "\n",
    "df2[(df2.cc>.75) & (df2.cc <1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train4 = train3.drop([\"above\", \"sqft_lot15\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome5 = \"price\"\n",
    "t5 = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'view',\n",
    "       'condition', 'grade', 'sqft_basement', 'yr_built', 'yr_renovated',\n",
    "       'zipcode', 'lat', 'long', 'wf', 'fliving', 'flot', 'basement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg(outcome=\"price\", x_cols=t5, df=train4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for correlation numerically\n",
    "df3=train4.corr().abs().stack().reset_index().sort_values(0, ascending=False)\n",
    "\n",
    "# zip the variable name columns (Which were only named level_0 and level_1 by default) in a new column named \"pairs\"\n",
    "df3['pairs'] = list(zip(df3.level_0, df3.level_1))\n",
    "\n",
    "# set index to pairs\n",
    "df3.set_index(['pairs'], inplace = True)\n",
    "\n",
    "#d rop level columns\n",
    "df3.drop(columns=['level_1', 'level_0'], inplace = True)\n",
    "\n",
    "# rename correlation column as cc rather than 0\n",
    "df3.columns = ['cc']\n",
    "\n",
    "# drop duplicates. This could be dangerous if you have variables perfectly correlated with variables other than themselves.\n",
    "# for the sake of exercise, kept it in.\n",
    "df3.drop_duplicates(inplace=True) \n",
    "\n",
    "df3[(df3.cc>.75) & (df3.cc <1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All previous trails were dropping columns based on high correlation scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial 6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train5 = train4.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train5.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome= \"price\"\n",
    "t6 = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'view',\n",
    "       'condition', 'grade', 'sqft_basement', 'yr_built', 'yr_renovated',\n",
    "       'zipcode', 'lat', 'long', 'wf', 'fliving', 'flot', 'basement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg(outcome=\"price\", x_cols=t6, df=train5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trail 7 \n",
    "Look for correlated columns to find a model with signifigance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train6 = train5.drop([\"zipcode\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train6.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look for correlated columns\n",
    "columns_correlations = []\n",
    "columns_non_numeric = []\n",
    "\n",
    "for column in mldf.drop(columns=[outcome]).columns:\n",
    "    try:\n",
    "        corr = np.abs(mldf[column].corr(mldf[outcome]))\n",
    "        t = (column, corr)\n",
    "        columns_correlations.append(t)\n",
    "    except:\n",
    "        columns_non_numeric.append(column)  \n",
    "        \n",
    "correlated_features_above_2 = [t[0] for t in columns_correlations if t[1] >= 0.10]\n",
    "correlated_features_above_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_correlations = []\n",
    "columns_non_numeric = []\n",
    "\n",
    "for column in train6.drop(columns=[outcome]).columns:\n",
    "    try:\n",
    "        corr = np.abs(train6[column].corr(train6[outcome]))\n",
    "        t = (column, corr)\n",
    "        columns_correlations.append(t)\n",
    "    except:\n",
    "        columns_non_numeric.append(column) \n",
    "columns_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_features_above_2 = [t[0] for t in columns_correlations if t[1] >= 0.10]\n",
    "correlated_features_above_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome=\"price\" \n",
    "t8 = ['bedrooms', 'bathrooms','floors', 'grade', 'lat','long','fliving', 'basement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg(outcome=\"price\", x_cols=t8, df=train4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my more correlated but more predictive model I think give there is a time and a place for this type of model it will be more predictived even though it is multicolinear. You could use both models in a buisness setting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take the sum of bathrooms/bedrooms \n",
    "train4[\"bed_bath\"] = train4['bedrooms'] + train4['bathrooms'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome=\"price\"\n",
    "t7 = ['fliving', 'grade', 'flot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I iterated over the combination of statistically signifigance correlations dozens of times. Most combinations hovered in the high 0.3 for the R2 scored. Many of the features in this data set were heavily correlated. I landed on the model below in part consideration of my stakehold looking at correlations that can be modified(renovated) in order to predict future home price. While it is not a perfect model I feel confident that any predictions I do make will not be do to multicollinearity and meets the assumptions of OLS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome=\"price\"\n",
    "t7 = ['fliving', 'grade', 'flot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg(outcome=\"price\", x_cols=t7, df=train6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression()\n",
    "\n",
    "linreg.fit(X_train, y_train)\n",
    "y_hat_test = linreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.concat([X_test, y_test], axis=1, join='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([X_train, y_train], axis=1, join='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_residuals = y_hat_train - y_test\n",
    "\n",
    "test_mse = mean_squared_error(y_test, y_hat_train)\n",
    "test_mse \n",
    "#sq root of this is 105837.68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_residuals = y_hat_train - y_test\n",
    "\n",
    "test_mse = mean_squared_error(y_test, y_hat_train)\n",
    "test_mse \n",
    "#sq root of this is 105837.68"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = make_scorer(mean_squared_error)\n",
    "\n",
    "cv_5_results = cross_val_score(linreg, X, y, cv=5, scoring=mse) \n",
    "cv_5_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_5_results  = np.mean(cross_val_score(linreg, X, y, cv=5,  scoring='neg_mean_squared_error'))\n",
    "cv_10_results = np.mean(cross_val_score(linreg, X, y, cv=10, scoring='neg_mean_squared_error'))\n",
    "cv_20_results = np.mean(cross_val_score(linreg, X, y, cv=20, scoring='neg_mean_squared_error')) \n",
    "\n",
    "print(cv_5_results)\n",
    "print(cv_10_results)\n",
    "print(cv_20_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All of these are relatively similar which is good! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.imshow(tts2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(tts2, center=0, annot=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset6.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = tts2\n",
    "\n",
    "# Split data into training and test splits\n",
    "train_idx, test_idx = train_test_split(df.index, test_size=.25, random_state=0)\n",
    "df['split'] = 'train'\n",
    "df.loc[test_idx, 'split'] = 'test'\n",
    "\n",
    "X = df[['view', 'grade']]\n",
    "y = df['price']\n",
    "X_train = df.loc[train_idx, ['view', 'grade']]\n",
    "y_train = df.loc[train_idx, 'price']\n",
    "\n",
    "# Condition the model on sepal width and length, predict the petal width\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "df['prediction'] = model.predict(X)\n",
    "\n",
    "fig = px.scatter(\n",
    "    df, x='price', y='prediction',\n",
    "    marginal_x='histogram', marginal_y='histogram',\n",
    "    color='split', trendline='ols'\n",
    ")\n",
    "fig.update_traces(histnorm='probability', selector={'type':'histogram'})\n",
    "fig.add_shape(\n",
    "    type=\"line\", line=dict(dash='dash'),\n",
    "    x0=y.min(), y0=y.min(),\n",
    "    x1=y.max(), y1=y.max()\n",
    ")\n",
    "\n",
    "fig.update_layout( \n",
    "    uniformtext_minsize=8, uniformtext_mode='hide',\n",
    "    title={\n",
    "        'text': \"Sqft_living & Grade\", \n",
    "        'y':0.95,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'}, \n",
    "    \n",
    "    xaxis=dict(\n",
    "        title='price',\n",
    "        titlefont_size=18,\n",
    "        tickfont_size=14, \n",
    "    ), \n",
    "    yaxis=dict(\n",
    "        title='prediction',\n",
    "        titlefont_size=18,\n",
    "        tickfont_size=14,\n",
    "    ),\n",
    "    \n",
    "    #paper_bgcolor='rgb(243, 243, 243)',\n",
    "    plot_bgcolor='rgb(243, 243, 243)',\n",
    "   \n",
    "    barmode='group',\n",
    "    bargap=0.15, # gap between bars of adjacent location coordinates.\n",
    "    bargroupgap=0.1, # gap between bars of the same location coordinate. \n",
    ") \n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mldf\n",
    "X = mldf.price[:, None]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, mldf, random_state=0)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "x_range = np.linspace(X.min(), X.max(), 100)\n",
    "y_range = model.predict(x_range.reshape(-1, 1))\n",
    "\n",
    "\n",
    "fig = go.Figure([\n",
    "    go.Scatter(x=X_train.squeeze(), y=y_train, name='train', mode='markers'),\n",
    "    go.Scatter(x=X_test.squeeze(), y=y_test, name='test', mode='markers'),\n",
    "    go.Scatter(x=x_range, y=y_range, name='prediction')\n",
    "])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_viz = mldf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_viz_no200k = fm_viz.loc[fm_viz['sqft_lot15'] < 50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_viz_no200k['sqft_lot15'].unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fm_viz_no200k\n",
    "fig = px.scatter(df, x=\"sqft_above\", y=\"price\",  facet_col=\"grade\", color=\"view\", trendline=\"ols\")\n",
    "fig.show()\n",
    "\n",
    "results = px.get_trendline_results(fig)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
